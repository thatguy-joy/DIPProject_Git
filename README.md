# From DICOM to Bitrate Matched ROI Compression: A Classical Pipeline for Liver Ultrasound

This repository implements a classical digital image processing pipeline that starts from liver ultrasound DICOM files and produces automatic lesion regions, region derived quality maps, bitrate matched JPEG compression outputs, and CSV plus PNG summaries.

The pipeline is implemented as a set of standalone Python scripts that read from and write to a `data/` directory. Most scripts assume a conventional folder layout under `data/`, and a few scripts include hard coded absolute paths that must be edited before running.

## What this project does

Given ultrasound DICOM studies placed under `data/raw/` ["Data is available here"](https://drive.google.com/drive/folders/1dcFbxS8MK-GCy3hG1u4kaQjIGN83CaLx?usp=drive_link), the pipeline can:

1. Scan DICOM tags and export them to a CSV.
2. Classify DICOMs as single frame vs multi frame (cine) and optionally delete cine files.
3. Extract PNG frames from DICOMs into `data/frames_full/`, including splitting dual panel images when present.
4. Detect on image calipers and produce debug overlays and wedge masks.
5. Generate automatic lesion masks guided by calipers and export lesion measurements.
6. Apply plausibility checks on lesion measurements.
7. Build a baseline Otsu segmentation and compare it with the automatic masks.
8. Morphologically refine masks and derive core, band, outer regions, including a radii ablation study.
9. Convert refined regions into continuous quality maps.
10. Run bitrate matched ROI focused JPEG compression and compute lesion restricted PSNR and SSIM.
11. Plot all key CSV outputs into report ready PNG figures.

## Repository structure

Scripts are currently at the repository root:

```
.
├─ src/
│  ├─ inspect_dicom_tags.py
│  ├─ filter_single_frame_dicoms.py
│  ├─ extract_frames_color.py
│  ├─ detect_calipers.py
│  ├─ auto_caliper_seg.py
│  ├─ check_area_plaus.py
│  ├─ seg_otsu_batch.py
│  ├─ compare_auto_vs_otsu.py
│  ├─ checkotsu_calipers.py
│  ├─ morph.py
│  ├─ morph_radius_ablation.py
│  ├─ quality_map.py
│  ├─ eval_intensity_transforms.py
│  ├─ eval_denoising.py
│  ├─ compression.py
│  └─ (other supporting scripts, if any)
│
├─ notebooks/
│  ├─ analyze_ablations.py
│  ├─ plot_morph.py
│  ├─ plot_compression_*.py
│  ├─ plot_seg_overlap_*.py
│  ├─ plot_intensity_*.py
│  └─ plot_denoising_*.py
│
└─ data/
   ├─ raw/                          # input DICOM studies placed here
   ├─ frames_full/                  # extracted PNG frames (reference images)
   ├─ detect_calipers_output/        # wedge masks + caliper debug overlays
   ├─ masks/                         # *_auto_mask.png generated by caliper guided segmentation
   ├─ bmode_frames/                  # overlay images produced during mask generation
   ├─ otsu_masks/                    # *_otsu_mask.png baseline masks
   ├─ morph_masks/                   # refined masks + region submasks
   │  ├─ morph_stats.csv
   │  ├─ morph_box.png
   │  └─ morph_scatter.png
   ├─ quality_maps/                  # quality maps and overlays (e.g., *_qmap.png, *_qoverlay.png)
   ├─ jpeg_out/                      # baseline JPEG and ROI focused JPEG outputs
   ├─ *.csv                          # metrics tables (compression, overlap, denoising, intensity, ablations)
   └─ *.png                          # plots/figures generated from the CSVs
```

Exact folder names matter because multiple scripts refer to them directly.

## Installation

### 1. Create and activate a virtual environment

```bash
python -m venv venv
# Windows PowerShell:
venv\Scripts\Activate.ps1
# macOS / Linux:
source venv/bin/activate
```

### 2. Install dependencies

Minimum dependencies used across scripts:

```bash
pip install numpy opencv-python pydicom imageio pandas matplotlib
```

If any script errors on missing packages, install what the traceback requests.

## Step by step execution

### Step 0. Put DICOMs in `data/raw/`

Place your input DICOM files under:

```
data/raw/
  <any nested folders you want>/
    *.dcm
```

Scripts recursively search under `data/raw/`.

---

### Step 1. Export DICOM tags (optional but recommended)

**Script:** `inspect_dicom_tags.py`
**Output:** a tags CSV under `data/` (filename is defined by the script)

```bash
python inspect_dicom_tags.py
```

Use this when you want a record of metadata such as the presence of multi frame series.

---

### Step 2. Classify single frame vs cine loop DICOMs (optional)

**Script:** `filter_single_frame_dicoms.py`
**Important:** this script can delete files depending on a `DRY_RUN` style flag in the script.

1. Open the script and ensure the safety flag is enabled first.
2. Run it to print the classification summary.
3. Only if you are satisfied, rerun with deletion enabled.

```bash
python filter_single_frame_dicoms.py
```

---

### Step 3. Extract PNG frames from DICOMs

**Script:** `extract_frames_color.py`
**Inputs:** `data/raw/`
**Outputs:** `data/frames_full/`

```bash
python extract_frames_color.py
```

This script normalizes frames to 8 bit and saves PNGs. It also contains logic to split dual panel images when present.

---

### Step 4. Detect calipers and produce wedge masks

**Script:** `detect_calipers.py`
**Inputs:** images in `data/frames_full/`
**Outputs:** `data/detect_calipers_output/`

Many caliper related scripts expect these intermediate results to exist.

```bash
python detect_calipers.py
```

If the script expects you to set an input path, edit the config area at the top or bottom of the file accordingly.

---

### Step 5. Generate auto lesion masks and lesion measurements

**Script:** `auto_caliper_seg.py`
**Inputs:** `data/frames_full/`
**Outputs:**

* `data/masks/` containing files ending with `_auto_mask.png`
* `data/bmode_frames/` overlay images
* `data/lesion_measurements.csv`

```bash
python auto_caliper_seg.py
```

This is the stage that produces the operational ROI masks used downstream.

---

### Step 6. Plausibility checks on lesion measurements

**Script:** `check_area_plaus.py`
**Inputs:** `data/lesion_measurements.csv`
**Outputs:** `data/lesion_measurements_plausibility.csv`

```bash
python check_area_plaus.py
```

This produces plausibility labels that later scripts may use to restrict evaluation.

---

### Step 7. Otsu baseline segmentation

**Script:** `seg_otsu_batch.py`
**Inputs:** frames and caliper guidance products as expected by the script
**Outputs:** `data/otsu_masks/` containing `_otsu_mask.png`

```bash
python seg_otsu_batch.py
```

---

### Step 8. Compare auto masks vs Otsu masks

**Script:** `compare_auto_vs_otsu.py`
**Inputs:** `data/masks/` and `data/otsu_masks/`
**Outputs:** `data/seg_overlap_auto_vs_otsu.csv`

```bash
python compare_auto_vs_otsu.py
```

Optional additional check:

**Script:** `checkotsu_calipers.py`
**Output:** `data/checkotsu_caliper.csv`

```bash
python checkotsu_calipers.py
```

---

### Step 9. Morphological refinement and derived regions

**Script:** `morph.py`
**Inputs:** `data/masks/`
**Outputs:**

* `data/morph_masks/` (refined masks and derived regions)
* `data/morph_stats.csv`

```bash
python morph.py
```

Optional ablation:

**Script:** `morph_radius_ablation.py`
**Outputs:** `data/morph_radius_ablation.csv`

```bash
python morph_radius_ablation.py
```

Then summarize ablation and produce plots:

**Script:** `analyze_ablations.py`
**Outputs:**

* `morph_radius_fractions_vs_radii.png`
* `morph_radius_perimeter_vs_radii.png`
* `data/morph_radius_ablation_summary.csv`

```bash
python analyze_ablations.py
```

---

### Step 10. Build quality maps

**Script:** `quality_map.py`
**Inputs:** `data/morph_masks/`
**Outputs:** `data/quality_maps/` including overlay images

```bash
python quality_map.py
```

---

### Step 11. Intensity transform and denoising evaluation (optional analyses)

These scripts produce CSV metrics and plots that end up in the report.

**Script:** `eval_intensity_transforms.py`
**Output:** `data/intensity_transform_metrics.csv`

```bash
python eval_intensity_transforms.py
```

**Script:** `eval_denoising.py`
**Output:** `data/denoising_metrics.csv`

```bash
python eval_denoising.py
```

Scripts can contains an absolute Windows path in a `DATA_DIR` style variable. Open the script and replace it with your local project path or a relative `Path("data")` style path before running.

---

### Step 12. ROI focused bitrate matched compression 

**Script:** `compression.py`
**Inputs:** frames, masks, and quality maps under `data/`
**Outputs:**

* `data/jpeg_out/` (JPEG outputs)
* `data/compression_metrics.csv`

```bash
python compression.py
```

---

### Step 13. Generate report ready PNG plots

Each plotting script reads one CSV and produces the figures used in the report.

```bash
python plot_seg_overlap.py
python plot_morph.py
python plot_intensity.py
python plot_denoising.py
python plot_compression.py
```

Expected figures include items like:

* `seg_overlap_dice_box.png`, `seg_overlap_iou_box.png`
* `morph_box.png`, `morph_scatter.png`
* `intensity_contrast_summary.png`, `intensity_cnr_summary.png`
* `denoising_cnr_summary.png`
* `compression_psnr_roi.png`, `compression_ssim_roi.png`,
  `compression_rel_bytes_diff_hist.png`, `compression_psnr_gain_vs_entropy_drop.png`

Exact filenames are defined inside each plotting script.

## Common issues and fixes

### Hard coded paths

Some scripts contain absolute paths (for example Windows drive paths). If a script fails with “file not found” but your folders are correct, open the script and replace the path constants with relative paths such as:

```python
from pathlib import Path
DATA_DIR = Path("data")
```

## Minimal run for reproducing the main report results

1. `extract_frames_color.py`
2. `detect_calipers.py`
3. `auto_caliper_seg.py`
4. `morph.py`
5. `quality_map.py`
6. `compression.py`
7. `plot_compression.py`